2021/07/31:
	SLAM 学习中期记录。今天尝试使用双目相机的左眼照片进行2d-2d的BRIEF描述子特征跟踪，但是程序输出的调试图像让我很疑惑，上一帧的图像总是和当前帧相同，这不应该，而且对于采用对极几何约束的2d方法，如果两帧之间的特征点没有平移量，那么求解无法初始化，也即无法获得本质矩阵E，因为涉及到的平移量t=0。后来经过仔细检查，发现问题出在图像的赋值上。
	OpenCV为了实现高效的存储方式，因此cv::Mat这种数据结构，使用=赋值时，只是将另外一个cv::Mat变量的内存指针，指向本cv::Mat的内存地址，下一次获取新帧写入该内存时，名为上一帧的变量实际上指向的是本帧图像内容，因此总是输出相同的两副图像。经过纠正后，R 和 t都得到了正确的求解(在相对尺度的误差范围内，这个误差可以接受，方向上尤其精确，但是整个相机的旋转位姿偶尔会出现奇怪的反转，推测是由于手持摄像头移动太快，缺少特征点求解)。预计下一步实现将位姿实施展示在pangolin窗口中，并尝试用多层的LK光流法进行特征点跟踪，以解决当物体表面纹理过少时缺少特征点的问题，另外经过研究实验发现，双目立体相机的SGBM块匹配求解，对于纹理丰富的表面还原效果很理想，对于普通光滑表面，还原效果交叉，拟采用八叉树概率更新来消除误差。
	
	关于realtime_plot:
	如果是重在定位，那么最重要的地方是位移的判断，经过实验发现，使用本质矩阵的方法计算出的旋转是比较符合实际情况的，但是位移有持续的噪声，也就是R较为准确，t噪声较多，经过调试输出发现，一般噪声在(0.45, 0.45, 0.45)附近波动，超过0.5的属于正常位移，但还未考虑到如何将实际值从带有噪声的数据中还原出来，也许在检测到真实位移后，直接在其上-0.45？
	现在仍未实现实时的定位与建图，或者说，可以实时地记录位姿，但是想通过pangolin这样的窗口实时展现出来还有点问题，尝试拖动鼠标改变视角则会引起程序崩溃(段错误)。
----------------------------------------------------------------------------
2021/08/3：
	创建一个线程默认的状态是joinable, 如果一个线程结束运行但没有被join,则它的状态类似于进程中的Zombie Process,即还有一部分资源没有被回收（退出状态码），所以创建线程者应该pthread_join来等待线程运行结束，并可得到线程的退出代码，回收其资源（类似于wait,waitpid)
但是调用pthread_join(pthread_id)后，如果该线程没有运行结束，调用者会被阻塞，在有些情况下我们并不希望如此，比如在Web服务器中当主线程为每个新来的链接创建一个子线程进行处理的时候，主线程并不希望因为调用pthread_join而阻塞（因为还要继续处理之后到来的链接），这时可以在子线程中加入代码
pthread_detach(pthread_self())
或者父线程调用
	pthread_detach(thread_id)（非阻塞，可立即返回，这将该子线程的状态设置为detached,则该线程运行结束后会自动释放所有资源
----------------------------------------------------------------------------
2021/08/07：
	隔了快三四天，重新回来学习初级SLAM系统，稍微读了一下《视觉SLAM十四讲》中，高翔博士实现的SLAM系统，把自己理清的结构记录在这里，希望之后学习其他算法，准备暑期实习的时间也能坚持写学习日记，加强学习印象。
	整个系统分为前端和后端
	前端负责：1.接受相机帧 
			2.利用双目相机的左眼图像，使用LK光流法对特征点进行追踪
			3.当特征点开始变少到某个阈值(高翔博士使用的阈值是50个点)，说明摄像机已经有了足	够的变化了，准备记录这个位置(变换矩阵，旋转矩阵)，以免错过位置无法建图，这个位置关联的帧称为关键帧，在系统开始运行到现在，只保留最新的7个关键帧以保证后端非线性优化和回环检测不会太耗时。
			
	后端负责：1.非线性优化，对关键帧估计的位姿和它们生成的世界坐标的地图点进行优化，产生一个关键帧即触发一次后端优化
			2.更新地图
			3.控制规模
	后端优化主要实现方式是g2o的BA(BundleAdjustment)优化
	
	前后端分两个线程执行，多数数据结构类使用std::shared_ptr<type>来管理新建，删除。MAP类持有Frame和Frame所对应的Features，即特征点，特征点怎么变为地图点呢？因为我们使用的设备是双目立体相机，所以，我们还没有利用右眼相机的图像。左眼相机通过光流法提取出运动跟踪的特征点，在右眼图像中找到这些对应的特征点，同时有双目立体相机的内外参，即可利用三角测距恢复特征点的深度，注意此时恢复的是像素的相机坐标，相机坐标到世界坐标还差一个变换矩阵变化。
	得想办法将这个系统运用到自己的数据集上，熟悉它的实现细节以后，将从数据集读取帧转换为从真实的双目摄像机中读取帧，目前老师借出的双目立体摄像头我已经标定好内外参，使用OpenCV自带的视差图计算函数(SGBM块匹配)也能获得不错的像素深度还原效果(如果有BA优化几乎就完美了，缺点是需要在光线和纹理丰富的地方，因为当时特征点提取没有使用LK光流法)
	另外有一些关于编译第13章SLAM系统时遇到的问题，编译该目标函数库，CMAKE_CXX_STANDARD需要显式地指定为14，因为g2o的库很多<>尖括号的写法只有C++14能正确识别，在C++11中是错误语法。另外，不知道高翔博士在编译其他静态库还是由于第三方静态库用到了so(shared object)共享库，那些共享库编译的时候可能由于Cmake的版本原因，没有显式指定-fPIC生成位置无关目标，导致很多其他用到这些so的库编译时报错(Relocation Error)即重定位错误，后来排查出，13章的代码有一个myslam的so库，编译选项是shared，想起自己平时都使用.a很少用到.so，试想如果用静态库是不是没有重定位问题，于是删除了shared选项，果然编译成功，故记录一下这段排查过程。
	最后是成功看到效果了，很震撼！第一次感受到数学和计算机的结合的威力，只是普普通通的照片居然能直接还原真实世界的环境和轨迹。但是还有改造的空间，13章的SLAM系统证明了光流法和BA优化是很好的组合，保持较小的优化规模即可，考虑到到时候机器人移动更慢，还需要额外的时间处理语义分割才行。星期天再好好研究一下BA优化和图优化，感觉非常重要，在小规模的数据集优化中能达到10ms级别的处理速度。
	接下来第一步是根据此系统改造为实时模块，第二步增加一个线程显示八叉树地图。
----------------------------------------------------------------------------
2021/08/11：
	今天又是看源码的一天，细细阅览高翔博士的SLAM系统愈发觉得精妙，对于线程的启动和使用掌控的非常精细。但是有些模块为什么那么划分还不是很清楚，比如特征点单独使用一个struct存储了很多信息而不只是记录特征点在每一帧的像素坐标，通过相机内参和位姿投影到世界坐标。而且config文件里面的相机内参根本没有用到呀？通过查阅资料了解到shared_ptr<type>是推荐的写法，同样unique_lock<std::lock>也是锁安全的，另外了解到了条件变量的使用，在这个系统中由于很多类都是public作用域，这个backend类的条件变量也是，用于被前端类frontend设置用来提醒在此条件变量上等待的锁(即后端优化线程)，使得线程能解锁继续运行，这个时候就启用了后端线程进行一次优化。
	还需要研究的有viewer可视化模块。这个模块将map类的数据显示在pangolin的窗口里。
	map地图类包含了所有帧，活动帧，所有点，活动点。
----------------------------------------------------------------------------
2021/08/12:
	开始动工，首先模仿Config类，实现一下配置文件读取，以及测试一下工程的构建是否有问题。了解到如果想使一个类成为单例(整个应用生命周期中只有一个对象)那么数据成员大多是static，并且构造函数声明为private私有的可以防止C++默认自动调用构造函数。
	尝试google的GTEST测试框架，对于这种大工程，最后实际运行总工程之前，应该对每个模块的函数进行测试，好在GTEST测试框架简单易用，稍微查阅一下资料即可上手使用，大概能提供类似Django这种网络web框架的单元测试效果。
----------------------------------------------------------------------------
2021/08/14：
	完成了对SLAM系统的config和camera类的模仿解析，写成了适应实时摄像的实现，进一步理解了高翔博士SLAM系统中camera类pose_的意义，因为使用的是双目摄像头(获取立体图像)，那么记录图像中的特征点要还原深度时就不能以单个摄像头的位姿还原，而是要将还原的起点先转换到双目摄像机中心，再将每一帧中的二维特征点坐标投射到世界坐标中。
	学习了gtest测试框架，开始上手了，发现做一个大工程之前对每一个模块进行测试非常有必要，这样能减少最终运行时debug的可能。
	目前还没有涉及到多线程，在之前的学习中已经试过多线程分别运行视里程记和位姿显示，因此打算将高翔博士的SLAM系统从双线程改写为三线程，可视化模块单独有一个线程实时运行，后端优化线程基本保持和高翔博士的SLAM系统一样，受到激活才运行。
----------------------------------------------------------------------------
2021/08/15:
	解决了两个奇怪的BUG，第一个是因为摄像头的USB线没有插稳，testcase总是过不了或者给出很奇怪的图像。第二个是由于OpenCV的相关知识不完备导致的。在我模仿高翔博士SLAM系统设计的过程中，我也设计了前端拥有单独的左右摄像机指针，然而，我忽略了一个问题，高翔博士的SLAM系统立体摄像机很可能是由两个单独的单目摄像头组成的，我的双目摄像头是自带集成芯片将两张图像合成输出的，通过简单的测试结果发现了OpenCV所在线程对摄像头等IO设备的控制权问题，如果在同一个线程中你尝试这样写：
	cv::VideoCapture caps[5];
    for (int i = 0; i < 5; ++i)
    {
        caps[i].open(2);
    }
那么就会报以下错误：
	[ WARN:0] VIDEOIO(V4L2:/dev/video2): can't open camera by index
	[ WARN:0] VIDEOIO(V4L2:/dev/video2): can't open camera by index
	[ WARN:0] VIDEOIO(V4L2:/dev/video2): can't open camera by index
	[ WARN:0] VIDEOIO(V4L2:/dev/video2): can't open camera by index
可以得出结论，同一个线程OpenCV只能拥有一个IO设备的一个指针。那这样的话我的camera类和frontend类可能需要大改了。
	同时再做一个备忘录，后面开始写map类后，frame一定要使用cv::Mat.clone()，不然很多过去帧的信息都会被覆写而丢失。
	弄懂了高翔博士SLAM系统中Frontend类的DtectFeatures函数，一开始疑惑这个函数是因为，使用gftt检测算法之前，创建了图像的掩码，把特征点所在的区域的方格十个像素内区域标记为不检测区域，这就很奇怪，为什么要把图像已经检测到的特征点屏蔽掉。后来看到下面一段代码：
	std::vector<cv::KeyPoint> keypoints;
    gftt_->detect(current_frame_->left_img_, keypoints, mask);
    int cnt_detected = 0;
    for (auto &kp : keypoints) {
        current_frame_->features_left_.push_back(
            Feature::Ptr(new Feature(current_frame_, kp)));
        cnt_detected++;
    }
也就是说如果不屏蔽之前存在的特征点，那么。。。
----------------------------------------------------------------------------
2021/08/17：
	今天的学习收获还是很多的，同时对CMake多目录编译注意事项也有了更多认识。首先今天是彻底弄明白高翔博士SLAM系统前端的运作方式了。之前看到检测特征点函数有屏蔽旧特征点区域再检测新帧的原理。因为前端大部分时候都在进行光流跟踪，也就是特征点不是每一帧都检测的！还是基础知识不牢固，光流法是对最初给定的点不断追踪的方法，如果在追踪途中，旧的特征点在新帧中不存在(可视化输出就是移出了窗口范围外)，那么这个特征点再没有手动对跟踪帧进行特征点检测补充之前，就永远的消失了。所以高翔博士的SLAM系统，随着镜头的移动，如果镜头移动距离足够远，在光流法追踪特征点的过程中就会逐渐丢失特征点，而每次调用前端的Track()都会记录当前有效的在地图中关联好的特征点，如果当前帧在地图中能找到的特征点小于80，说明镜头移动足够远了，应该插入新的关键帧。InsertKeyFrame()中就会对新来的这一帧进行所有特征点检测，在这里，特征点被补充了，然后再次随着光流法的不断追踪，特征点逐渐丢失，周而复始往复循环，就完成了前端的追踪。但是高翔博士前端追踪的代码里面还包含了一个小型的优化过程，就是在当前帧匹配已有地图点时，目前还没完全弄清楚这个意义，可能是为了误差匹配，也可能是优化求解位姿(我猜是前者，因为这一段代码中并没有特征点三角化匹配地图点的过程)
	另外就是在理解了上述代码后，编译CMake工程又出了问题，在编译总工程的时候会出现这个错误：
	undefined reference to `Camera::Camera(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int)'
	意思是没有在链接库中找到这个函数签名，当时感觉很奇怪，因为使用同一个链接库的测试单元编译成功而且可以成功运行，在网上到处查阅资料和博客，大多数情况都指出，链接库的编译版本和运行程序cpp的编译版本不一样，正当我考虑要不要换GCC版本时，我突然想到，既然测试单元能正常编译，那么当时我测试单元的CMake是怎么写的呢？经过对比发现，总工程文件cpp的CMake链接库语句中只链接了第三方库，没有链接我之前编译的自己的SLAM库。。。无语。
	
	测试单元的CMake：
	TARGET_LINK_LIBRARIES(${test_src} ${THIRD_PARTY_LIBS} myslam)
	总工程的CMake：
	TARGET_LINK_LIBRARIES(lk_stereo ${THIRD_PARTY_LIBS})
	
	我一直以为根目录下CMakeLists.txt包含的子目录的CMakeLists.txt，各个子目录的编译会受到之前已经编译的目录的影响，结果并不是，所有子目录是单独编译的，所以每个子目录的工程文件都需要显式的链接需要的库，为了找出这个问题硬是花了一个钟。。。
