2021/07/31:
	SLAM 学习中期记录。今天尝试使用双目相机的左眼照片进行2d-2d的BRIEF描述子特征跟踪，但是程序输出的调试图像让我很疑惑，上一帧的图像总是和当前帧相同，这不应该，而且对于采用对极几何约束的2d方法，如果两帧之间的特征点没有平移量，那么求解无法初始化，也即无法获得本质矩阵E，因为涉及到的平移量t=0。后来经过仔细检查，发现问题出在图像的赋值上。
	OpenCV为了实现高效的存储方式，因此cv::Mat这种数据结构，使用=赋值时，只是将另外一个cv::Mat变量的内存指针，指向本cv::Mat的内存地址，下一次获取新帧写入该内存时，名为上一帧的变量实际上指向的是本帧图像内容，因此总是输出相同的两副图像。经过纠正后，R 和 t都得到了正确的求解(在相对尺度的误差范围内，这个误差可以接受，方向上尤其精确，但是整个相机的旋转位姿偶尔会出现奇怪的反转，推测是由于手持摄像头移动太快，缺少特征点求解)。预计下一步实现将位姿实施展示在pangolin窗口中，并尝试用多层的LK光流法进行特征点跟踪，以解决当物体表面纹理过少时缺少特征点的问题，另外经过研究实验发现，双目立体相机的SGBM块匹配求解，对于纹理丰富的表面还原效果很理想，对于普通光滑表面，还原效果交叉，拟采用八叉树概率更新来消除误差。
	
	关于realtime_plot:
	如果是重在定位，那么最重要的地方是位移的判断，经过实验发现，使用本质矩阵的方法计算出的旋转是比较符合实际情况的，但是位移有持续的噪声，也就是R较为准确，t噪声较多，经过调试输出发现，一般噪声在(0.45, 0.45, 0.45)附近波动，超过0.5的属于正常位移，但还未考虑到如何将实际值从带有噪声的数据中还原出来，也许在检测到真实位移后，直接在其上-0.45？
	现在仍未实现实时的定位与建图，或者说，可以实时地记录位姿，但是想通过pangolin这样的窗口实时展现出来还有点问题，尝试拖动鼠标改变视角则会引起程序崩溃(段错误)。
----------------------------------------------------------------------------
2021/08/3：
	创建一个线程默认的状态是joinable, 如果一个线程结束运行但没有被join,则它的状态类似于进程中的Zombie Process,即还有一部分资源没有被回收（退出状态码），所以创建线程者应该pthread_join来等待线程运行结束，并可得到线程的退出代码，回收其资源（类似于wait,waitpid)
但是调用pthread_join(pthread_id)后，如果该线程没有运行结束，调用者会被阻塞，在有些情况下我们并不希望如此，比如在Web服务器中当主线程为每个新来的链接创建一个子线程进行处理的时候，主线程并不希望因为调用pthread_join而阻塞（因为还要继续处理之后到来的链接），这时可以在子线程中加入代码
pthread_detach(pthread_self())
或者父线程调用
	pthread_detach(thread_id)（非阻塞，可立即返回，这将该子线程的状态设置为detached,则该线程运行结束后会自动释放所有资源
----------------------------------------------------------------------------
2021/08/07：
	隔了快三四天，重新回来学习初级SLAM系统，稍微读了一下《视觉SLAM十四讲》中，高翔博士实现的SLAM系统，把自己理清的结构记录在这里，希望之后学习其他算法，准备暑期实习的时间也能坚持写学习日记，加强学习印象。
	整个系统分为前端和后端
	前端负责：1.接受相机帧 
			2.利用双目相机的左眼图像，使用LK光流法对特征点进行追踪
			3.当特征点开始变少到某个阈值(高翔博士使用的阈值是50个点)，说明摄像机已经有了足	够的变化了，准备记录这个位置(变换矩阵，旋转矩阵)，以免错过位置无法建图，这个位置关联的帧称为关键帧，在系统开始运行到现在，只保留最新的7个关键帧以保证后端非线性优化和回环检测不会太耗时。
			
	后端负责：1.非线性优化，对关键帧估计的位姿和它们生成的世界坐标的地图点进行优化，产生一个关键帧即触发一次后端优化
			2.更新地图
			3.控制规模
	后端优化主要实现方式是g2o的BA(BundleAdjustment)优化
	
	前后端分两个线程执行，多数数据结构类使用std::shared_ptr<type>来管理新建，删除。MAP类持有Frame和Frame所对应的Features，即特征点，特征点怎么变为地图点呢？因为我们使用的设备是双目立体相机，所以，我们还没有利用右眼相机的图像。左眼相机通过光流法提取出运动跟踪的特征点，在右眼图像中找到这些对应的特征点，同时有双目立体相机的内外参，即可利用三角测距恢复特征点的深度，注意此时恢复的是像素的相机坐标，相机坐标到世界坐标还差一个变换矩阵变化。
	得想办法将这个系统运用到自己的数据集上，熟悉它的实现细节以后，将从数据集读取帧转换为从真实的双目摄像机中读取帧，目前老师借出的双目立体摄像头我已经标定好内外参，使用OpenCV自带的视差图计算函数(SGBM块匹配)也能获得不错的像素深度还原效果(如果有BA优化几乎就完美了，缺点是需要在光线和纹理丰富的地方，因为当时特征点提取没有使用LK光流法)
